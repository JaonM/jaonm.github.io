---
layout:     post   				    # 使用的布局（不需要改）
title:      语言模型学习笔记 				# 标题 
subtitle:   n-gram neural network language model #副标题
date:       2018-08-28 				# 时间
author:     MA Qiang 						# 作者
header-img: img/post-bg-universe.jpg 	#这篇文章标题背景图片
catalog: true 						# 是否归档
tags:								#标签
    - n-gram
    - language model
    - note
    
---
## 背景
语言模型是nlp中重要的组成部分，其在对话系统、输入法联想词、机器翻译中都有应用。
## 概念
语言模型简单来说就是用来判断一个句子概率，判断其是否说人话。
## 统计语言模型
句子 $s=w_1w_2w_3...w_l $ 其概率公式表示为
$$
\begin{align}
p(s)&=p(w_1)p(w_2|w_1)p(w_3|w_1w_2)...p(w_l|w_1w_2...w_{l-1}) \\
&=\prod_{i=1}^lp(w_i|w_1...w_i-1)\tag{1}
\end{align}
$$
这样就带来了两个问题： 

1. 参数空间太大，在公式$(1)$中，计算第$i$个词的概率需要考虑前$i-1$种情况，这就会带来$\vert{V}\vert^{i-1}$的指数规模的参数空间大小，$\vert{V}\vert$为词库大小；

2. 数据系数，有可能绝大多数词对并没有出现在历史语料当中，对应概率为0，需要平滑处理。

    

###  马尔可夫假设与ngram

为了解决上述参数太大的问题，引入马尔可夫假设，即一个词出现的概率只与前$n-1$个词有关。

- 当$n=1$时，表示每个词相互独立，也称为unigram，此时句子$s$ 的概率表示为：

- $$
  p(s)=p(w_1)p(w_2)...p(w_l)\tag{2}
  $$

- 当$n=2$时，表示每个词与前一个词有关，称为bigram (2-gram)，此时句子$s$的概率为:
  $$
  p(s)=p(w_1|<BOS>)p(w_2|w1)...p(w_l|w_{l-1})p(<EOS>|w_l)\tag{3}
  $$
  <BOS>和<EOS>分别标记开始符和结束符

- 当$n=3$时，表示每个词与前两个词有关，称为trigram (3-gram)，此时句子$s$的概率为:
  $$
  p(s)=p(w_1|<BOS>)p(w_2|w_1,<BOS>>p(w_3|w_1,w_2))...p(w_l|w_{l-1},w_{l-2})\tag{3}
  $$

- 对于$n>2$的ngram语言模型，我们定义$p(s)=\prod_{i=1}^{l+1}p(w_i|w_{i-n+q}^{i-1})$

比较常用的是bigram和trigam四元组或者更高远组由于需要更多的语料，数据系数严重并且提升精度不多，很少使用。

为了计算$p(w_i|w_{i-1})$的概率，可以简单计算二元祖$(w_{i-1}w_{i})$在语料中的频率，根据条件概率得到如下公式:
$$
p(w_i|w_{i-1})=\frac{count(w_{i-1}w_i)}{count(w_{i-1})}\tag{5}
$$

### 一个栗子

给一段预料$S$由下面三句例子组成：

<center>
    "Brown read holy bible",<br>
    "Mark read a text book",<br>
    "He read a book by David"
</center>

现在用bigram计算$p($"Brown read a book"$)$的值:
$$
\begin{align}
p(Brown|<BOS>)&=\frac{count(<BOS>,Brown)}{count(BOS)}=\frac{1}{3}\\
p(read|Brown)&=\frac{count(Brown,read)}{count(Brown)}=1\\
p(a|read)&=\frac{count(read,a)}{count(read)}=\frac{2}{3}\\
p(book|a)&=\frac{count(a,book)}{count(a)}=\frac{1}{2}\\
p(<EOS>|book)&=\frac{count(book,EOS)}{count(book)}=\frac{1}{2}
\end{align}
$$

$$
\begin{align}
p &=p(Brown|<BOS>)\times p(read|Brown)\times p(read|Brown)\times p(a|read)p(book|a)\times p(<EOS>|book)\\
&=\frac{1}{3}\times1\times\frac{2}{3}\times\frac{1}{2}\times\frac{1}{2}\\
&=\frac{1}{18}\approx0.06
\end{align}
$$

### 性能评价

我们可以用交叉熵 (cross-entropy)和困惑度 (perlexity)来评价一个语言模型的好坏。

给定文本T是由句子$(t_1,t_2,...,t_{l_T})$,计算文本T的概率为：
$$
p(T)=\prod_{i=1}^{l_T}p(t_i)\tag{6}
$$
就是将所有所有句子的概率乘起来。

在数据$T$上模型$p(w_i|w^{i-1}_{n-i+})$的交叉熵定义为
$$
H_p(T)=-\frac{1}{W_T}\log_2p(T)\tag{7}
$$
其中$W_T$表是文本$T$中单词的长度，从信息论的角度看，公式$(7)$可以解释为利用与模型$p(w_i|w^{i-1}_{n-i+})$有关的压缩算法对数据集合中的$W_T$个词进行编码，每一个编码所需要的平均比特位数。

另外困惑度定义为
$$
 PP_T(T)=2^{H_p(T)}\tag{8}
$$
交叉熵和困惑度越小，表示模型越好。

### 数据平滑

此时模型存在一个问题，假设利用上述预料$S$计算句子"David read a book" ，有
$$
p(read|David)=\frac{count(David,Read)}{count(David)}=\frac{0}{1}
$$
即$p($David read a book$)=0$，显然这个概率是有问题的，因为这个句子是有可能成立的，对于语料中没有对应词对的情况，我们需要进行数据平滑，下面介绍几个平滑的方法。

#### 加法平滑法

加法平滑法(addictive smoothing)是最简单的一种平滑技术，其公式为
$$
p_{add}(w_i|w_{i-n+1}^{i-1})=\frac{\delta+count(w^i_{i-n+1})}{\delta\times\vert{V}\vert+count(w_{i-n+1}^{i-1})}\tag{9}
$$
其中$ 0\leq \delta \leq1$，是一个可变参数。

#### 古德-图灵(Good-Turing)估计法

其基本思路是，对于出现$r$次的n元语法，都假设其出现了$r^*$次，其中
$$
r^*=(r+1)\frac{n_{r+1}}{n_r}\tag{10}
$$
$n_r$表示训练语料中刚好出现$r$次的n元语法数目，要把统计数化为概率需要进行归一化
$$
p_r=\frac{r^*}{N}\tag{11}
$$
其中
$$
N=\Sigma^\infty_{r=0}n_rr^*=\Sigma^\infty_{r=0}(r+1)n_{r+1}=\Sigma^\infty_{r=1}n_rr\tag{12}
$$
具体$r^*$公式推导可以参考文献[Nadas,1985;Chen and Goodman,1998]。

还有其他很多平滑方法如Katz平滑方法，Jelinek-Mercer平滑方法等这里暂不做详细介绍，留个坑。



## 神经网络语言模型

由于n-gram语言模型有很强的约束性即一个词只与它前n个词有关，往往与实际情况不合，于是乎有学者提出了神经网络语言模型。

### 神经概率语言模型 (Neural Probabilistic Language Model)

Neural Probabilistic Language Model 出自论文(Bengio et. 2003)，引入神经网络来构建语言模型。

给定由$w_1,w_2,...,w_t$的单词序列组成的训练语料，我们的目标是训练模型$f$
$$
f(w_t,...,w_{t-n+1})=\hat{P}(w_t|^{t-1}_1)\tag{13}
$$
预测第$t$个词的概率。

#### 词嵌入

文章中提出了一个映射$C$，也就是我们熟悉的词嵌入矩阵，将$i-th$个词映射称为$m$维特征向量(feature vectors)，映射$C$是一个$\vert{V}\vert\times m$大小的矩阵，同时也为后来著名的word2vec埋下伏笔。

#### 目标函数

$$
L=\frac{1}{T}\Sigma_tlogf(w_t,w_{t-1},...,w_{t-n+1};\theta)+R(\theta)\tag{14}
$$

其中 $R(\theta)$是一个权重衰减(weight decay)罚项，作用于映射$C$和网络权重。

#### 网络结构

![模型图](https://ws1.sinaimg.cn/large/0069RVTdgy1furt8ytz4aj31ak0umwhu.jpg)

<center>神经概率语言模型结构图</center>

不算词嵌入层，模型是一个只含有一个隐层的浅层神经网络。

#### 计算过程

1. $w_{t-1},...,w_{t-n+1}$通过映射$C$转化为词向量$C(w_{t-1}),C(w_{t-2}),...,C(w_{t-n+1})$；
2. 通过拼接得到向量$x$;
3. 进行计算$y=b+Wx+U\tanh(d+Hx)$，$y\in R^{\vert{V}\vert}$；
4. 通过计算softmax，$\frac{e^{y_{w_t}}}{\Sigma_ie^{y_i}}$得到模型输出，即$\hat{P}(w_t|w_{t-1},..,w_{t-n+1})$。

其中，

$H\in R^{h\times(n-1)m},U\in R^{\vert{V}\vert\times h},W\in R^{\vert{V}\vert\times(n-1m)}$，$h$为隐层单元数量，$d$是隐层bias，$b$是输出层bias。

#### 训练过程

随机梯度上升 (stochastic gradient ascent)
$$
\theta\leftarrow\theta+\epsilon\frac{\partial\log\hat{P}(w_t|w_{t-1},...,w_{t-n+1})}{\partial\theta}\tag{15}
$$

#### 不足

- 训练上下文长度固定
- 有限的窗口大小

### 循环神经网络语言模型 (Recurrent neural network language model)

#### 概念

为了解决前馈神经网络 (feed-forward neural network)只能对固定长度上下文建模的问题，Tomas Mikolov等人提出了用RNN进行语言模型建模。

#### 模型

![rnn_lm](https://ws2.sinaimg.cn/large/0069RVTdgy1fusqiz19hbj30pp0903ys.jpg)

<center>RNN 语言模型结构图</center>

涉及到如下几个运算
$$
\begin{align}
x(t)&=concatenate(w(t),s(t-1)) \tag{16}\\
s(t)&=f(Ux(t))\tag{17}\\
y(t)&=g(Vs(t))\tag{18}
\end{align}
$$
其中，$w(t)$是one-hot encode之后的向量，$f$是sigmoid激活函数,$g$是softmax激活函数。

#### 参数初始化

初始隐层$s(0)$用一些小数字初始化，隐层大小通常取30-500 (数据量越大，值越大)，$U,V$网络权重用高斯分布初始化 ($mean=0.1 variance=1$)

#### 训练

- 训练函数是交叉熵损失函数。

- 利用[BPTT](http://www.cnblogs.com/zhbzz2007/p/6339346.html)算法进行训练，较详细的介绍见下小结。

- 学习率初始值为0.1，随着训练过程进行衰减。

- 文章提出了一个动态模型的概念 (dynamic model)，即模型在测试过程中同时将测试数据加入训练过程，更新模型参数。

#### BPTT

RNN模型的基本公式由公式(16)、(17)、(18)组成，我们的目标是求得矩阵$U，V$中的梯度更新参数。

以分类问题为例，常用交叉熵$L = -\Sigma_iy_i\ln p(x_i)$ 作为损失函数，但是RNN需要在每个时间步$t$中计算损失函数，因此我们定义RNN 的损失函数为
$$
\begin{align}
L_t(y_t,\hat{y_t})&=-\Sigma_k y_{tk}\log(\hat{y_{tk}})\\
L &= \Sigma_tL_t(y_t,\hat{y_t})=-\Sigma_t\Sigma_ky_{t}log(\hat{y_{tk}})\tag{19}
\end{align}
$$
其中，$\hat{y_{tk}=softmax(z_{tk})=\frac{exp(z_{tk})}{\Sigma_i^Kexp(z_{ti})}}$，$z(t)=Vs(t)$

以$t=3$为例，根据链式法则，
$$
\begin{align}
\frac{\partial L_3}{\partial V}&=\frac{\partial L_3}{\partial{\hat{y_{3}}}}\frac{\partial\hat{y_3}}{\partial z_3}\frac{\partial z_3}{\partial V}\\
&=-\Sigma_i\frac{y_k}{\hat{y_k}}(a_k(1-a_k)-\Sigma_{k\neq i}a_ka_i)\bigotimes s_3\\
&=\hat{y_3}-y3\tag{20}
\end{align}
$$
对于$\frac{\partial L_3}{\partial U}$这种情况就需要考虑到历史情况了，因为它不仅和当前时间步的$s_t$有关，还和历史时间步$s_{t-1},s_{t-2}...$有关，得到如下公式
$$
\frac{\partial L_3}{\partial U}=\Sigma_{t=0}^{3}\frac{\partial L_3}{\partial \hat{y_3}}\frac{\partial y_3}{\partial s_3}\frac{\partial s_3}{\partial s_t}\frac{\partial s_t}{\partial U}\tag{21}
$$


#### 不足

- 文章没有运用词嵌入而是直接用one-hot表示每个单词，难免会导致数据系数
- 文章采用Simple RNN去训练，会存在梯度消失或者梯度爆炸的问题

### RNN语言模型加强版 (Extensions of RNN Language Model)

Tomas Mikolov 等人认为之前提出的RNN模型性能效果已经比较理想，但是模型速度比较慢，因此这篇文章主要探讨怎么加速模型训练和预测过程。

#### 概念

每一轮训练所需要的时间复杂度为
$$
O =(1+H)\times H \times\tau+H\times V \tag{22}
$$
其中$H$表示隐层大小，$V$表示词库大小，$\tau$表示反向传播中的steps，通常来说$H\ll V$，可见模型的计算瓶颈在隐层和输出层。

文章假设每个单词 (unigram)属于某个特定的类别 (class)，我们可以估计每个类别的历史概率分布并通过在这个类别下单词的条件概率，即
$$
P(w_i|history)=P(c_i|history)P(w_i|c_i)\tag{23}
$$
此时模型时间复杂度下降为
$$
O=(1+H)\times H\times \tau+H\times C \tag{24}
$$
其中$C$表示类别的大小。

#### 进一步假设

我们假设每个单词的概率既依赖于单词的类别也依赖于历史上下文，即公式(23)改写为
$$
P(w_i|history)=P(c_i|\boldsymbol s(t))P(w_i|c_i,\boldsymbol s(t))\tag{25}
$$
对应的网络架构变为

![extended_rnn](https://ws1.sinaimg.cn/large/0069RVTdgy1fuweegooz4j30us0l43zq.jpg)

<center>改变后的网络架构</center>

将词语按照词频进行排序，类别的大小是一个超参数，假如分成20类，前5%为第一类，下一个5%为第二类，以此类推，可以看出最后一个5%都是一些低频词。

#### 两个公式

$$
\begin{align}
c_t(t)&=softmax(s(t)W)\tag{26}\\
y_c(t)&=softmax(s(t)V)\tag{27}\\
\end{align}
$$
对应公式(25)中的$P(c_t|history)$和$P(w_i|c_i,\boldsymbol s(t))$，单词的概率$P(w_i|history)$可以求出。

### 字符级语言模型 (Character-Aware Neural Language Model)

#### 概念

模型的输入以字符为单位，输出以单词为单位，模型本质类似一个RNN模型，不过每个时间步的组成将CNN+Pooling、Highway Network、LSTM、Sotfmax这些结构串联起来作为模型的基本结构。

#### 网络结构

![char-nnlm](https://ws2.sinaimg.cn/large/006tNbRwgy1fuxjpn3ivqj30gy0ngtaw.jpg)

<center>character-aware neural language model结构图</center>

模型的结构相对清晰，接下来以从下往上的顺序分别介绍每层结构。

#### Character embedding

设$C$是字符的集合，$l$是单词长度，$d$是character embedding的维度，文章假设$d<\vert{C}\vert$ (作者发现低维度的向量表示比原始表示如:one-hot性能表现较好)，因此单词$k$可以用一个矩阵表示、即$C^k \in R^{d\times l}$。

于是乎模型的输入是一个4维张量(batch_size,sequence_length,l,d)。

#### Convolution layer

文章采用窄卷积 (narrow convolution) filter大小为$H\in R^{d\times w}$对 $ C^k$进行卷积操作，得到大小为 $\boldsymbol{f}^k\in R^{l-w+1}$的feature map，有如下公式
$$
\boldsymbol{f}^k[i]=\tanh(<C^k[*,i:i+w-1],H>+b)\tag{28}
$$
其中，$C^k[*,i:i+w-1]$表示矩阵$C^k$中的$i$到$i+w-1$列，$<A,B>$是Frobenius inner product。

将多个大小相同的filter通过concatenation操作后输入*Max-over-time pooling layer*，得到输出向量$y^k$。
$$
y^k=\max_i \boldsymbol{f}^k[i]\tag{29}
$$
这样做的目的是在由多个字符序列组成单词中捕获到最重要的特征。

多个大小不同的filter其实就是多个ngram提取器，提取不同ngram 大小特征。

#### Highway Network

Highway Network最主要的概念将RNN变体中的门概念应用到前馈神经网络当中，相比于普通的前馈神经网络
$$
\boldsymbol{z}=g(\boldsymbol{W}\boldsymbol{y}+\boldsymbol{b})\tag{30}
$$
Highway Network 多了transform gate 和 carry gate的概念。
$$
\boldsymbol{z}=\boldsymbol{t}\bigodot g(\boldsymbol{W}_H\boldsymbol{y}+\boldsymbol{b}_H)+(\boldsymbol{1}-\boldsymbol{y})\bigodot\boldsymbol{y}\tag{31}
$$
其中，$\boldsymbol{t}=\sigma(\boldsymbol{W}_T \boldsymbol{y}+\boldsymbol{b}_T)$，称作transform gate，$\boldsymbol{1}-\boldsymbol{t}$称作carry gate。

这样设计的Highway Network 允许模型可以进行更深的叠加，一定程度上缓解了梯度消失的问题。

#### Long Short-Term Memory Network

LSTM是RNN的一种变体，通过门控单元和Memory Cell的设计解决RNN训练中带来的梯度消失问题，这里不在详细介绍，想必大家都比较熟悉，想详细了解的同学可以参考[这篇](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)文章，这里列举主要公式
$$
\begin{align}
\boldsymbol{i}_t&=\sigma(\boldsymbol{W^i}\boldsymbol{x}_t+\boldsymbol{U}^i\boldsymbol{h}_{t-1}+\boldsymbol{b}^i) \\
\boldsymbol{f}_t&=\sigma(\boldsymbol{W}^f\boldsymbol{x}_t+\boldsymbol{U}^f\boldsymbol{h}_{t-1}+\boldsymbol{b}^f)\\
\boldsymbol{o}_t&=\sigma(\boldsymbol{W}^o\boldsymbol{x}_t+\boldsymbol{U}^o\boldsymbol{h}_{t-1}+\boldsymbol{b}^o)\\
\boldsymbol{g}_t&=\tanh(\boldsymbol{W}^g\boldsymbol{x}_t+\boldsymbol{U}^g\boldsymbol{h}_{t-1}+\boldsymbol{b}^g)\\
\boldsymbol{c}_t&=\boldsymbol{f}_t\bigodot\boldsymbol{c}_{t-1}+\boldsymbol{i}_t\bigodot\boldsymbol{g}_t\\
\boldsymbol{h}_t&=\boldsymbol{o}_t\bigodot\tanh(\boldsymbol{c}_t)\tag{32}
\end{align}
$$

#### 损失函数和训练

采用交叉熵作为损失函数和BPTT算法的随机梯度下降(SGD)。

## 参考

- 统计自然语言处理 第2版 宗成庆著
- Bengio, Yoshua, et al. "A neural probabilistic language model." Journal of machine learning research 3.Feb (2003): 1137-1155.
- Mikolov T, Karafiát M, Burget L, et al. Recurrent neural network based language model[C]//Eleventh Annual Conference of the International Speech Communication Association. 2010.
- http://friskit.me/2016/10/18/translation-wildml-recurrent-neural-networks-tutorial-part-3-bptt/
- Mikolov, Tomáš, et al. "Extensions of recurrent neural network language model." Acoustics, Speech and Signal Processing (ICASSP), 2011 IEEE International Conference on. IEEE, 2011.
- Kim, Yoon, et al. "Character-Aware Neural Language Models." AAAI. 2016.